# -*- coding: utf-8 -*-
"""AI_TeamProcject_yolo404.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcSc7WiDktI5SCx5RRA1WKsaxHAcVPHs
"""



#YOLO4 사용을 위한 환경  확인
# CUDA: Let's check that Nvidia CUDA drivers are already pre-installed and which version is it.
!/usr/local/cuda/bin/nvcc --version

!nvidia-smi

#YOLO4 사용을 위한 환경  확인
import os
os.environ['GPU_TYPE'] = str(os.popen('nvidia-smi --query-gpu=name --format=csv,noheader').read())

def getGPUArch(argument):
  try:
    argument = argument.strip()
    # All Colab GPUs
    archTypes = {
        "Tesla V100-SXM2-16GB": "-gencode arch=compute_70,code=[sm_70,compute_70]",
        "Tesla K80": "-gencode arch=compute_37,code=sm_37",
        "Tesla T4": "-gencode arch=compute_75,code=[sm_75,compute_75]",
        "Tesla P40": "-gencode arch=compute_61,code=sm_61",
        "Tesla P4": "-gencode arch=compute_61,code=sm_61",
        "Tesla P100-PCIE-16GB": "-gencode arch=compute_60,code=sm_60"

      }
    return archTypes[argument]
  except KeyError:
    return "GPU must be added to GPU Commands"
os.environ['ARCH_VALUE'] = getGPUArch(os.environ['GPU_TYPE'])

print("GPU Type: " + os.environ['GPU_TYPE'])
print("ARCH Value: " + os.environ['ARCH_VALUE'])

STEP 1. Install cuDNN according to the current CUDA
version Colab added cuDNN as an inherent install - so you don't have to do a thing - major win

Step 2: Installing Darknet for YOLOv4 on Colab

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/
# %rm -rf darknet

!git clone https://github.com/roboflow-ai/darknet.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/darknet/
# %rm Makefile

# Commented out IPython magic to ensure Python compatibility.
# #colab 환경에서 darknet 구축을 위한 환경설정 진행 (cpu -> gpu로 변경)
# %%writefile Makefile
# GPU=1
# CUDNN=1
# CUDNN_HALF=0
# OPENCV=1
# AVX=0
# OPENMP=0
# LIBSO=1
# ZED_CAMERA=0
# ZED_CAMERA_v2_8=0
# 
# # set GPU=1 and CUDNN=1 to speedup on GPU
# # set CUDNN_HALF=1 to further speedup 3 x times (Mixed-precision on Tensor Cores) GPU: Volta, Xavier, Turing and higher
# # set AVX=1 and OPENMP=1 to speedup on CPU (if error occurs then set AVX=0)
# # set ZED_CAMERA=1 to enable ZED SDK 3.0 and above
# # set ZED_CAMERA_v2_8=1 to enable ZED SDK 2.X
# 
# USE_CPP=0
# DEBUG=0
# 
# ARCH= -gencode arch=compute_35,code=sm_35 \
#       -gencode arch=compute_50,code=[sm_50,compute_50] \
#       -gencode arch=compute_52,code=[sm_52,compute_52] \
# 	    -gencode arch=compute_61,code=[sm_61,compute_61] \
#       -gencode arch=compute_37,code=sm_37
# 
# ARCH= -gencode arch=compute_60,code=sm_60
# 
# OS := $(shell uname)
# 
# VPATH=./src/
# EXEC=darknet
# OBJDIR=./obj/
# 
# ifeq ($(LIBSO), 1)
# LIBNAMESO=libdarknet.so
# APPNAMESO=uselib
# endif
# 
# ifeq ($(USE_CPP), 1)
# CC=g++
# else
# CC=gcc
# endif
# 
# CPP=g++ -std=c++11
# NVCC=nvcc
# OPTS=-Ofast
# LDFLAGS= -lm -pthread
# COMMON= -Iinclude/ -I3rdparty/stb/include
# CFLAGS=-Wall -Wfatal-errors -Wno-unused-result -Wno-unknown-pragmas -fPIC
# 
# ifeq ($(DEBUG), 1)
# #OPTS= -O0 -g
# #OPTS= -Og -g
# COMMON+= -DDEBUG
# CFLAGS+= -DDEBUG
# else
# ifeq ($(AVX), 1)
# CFLAGS+= -ffp-contract=fast -mavx -mavx2 -msse3 -msse4.1 -msse4.2 -msse4a
# endif
# endif
# 
# CFLAGS+=$(OPTS)
# 
# ifneq (,$(findstring MSYS_NT,$(OS)))
# LDFLAGS+=-lws2_32
# endif
# 
# ifeq ($(OPENCV), 1)
# COMMON+= -DOPENCV
# CFLAGS+= -DOPENCV
# LDFLAGS+= `pkg-config --libs opencv4 2> /dev/null || pkg-config --libs opencv`
# COMMON+= `pkg-config --cflags opencv4 2> /dev/null || pkg-config --cflags opencv`
# endif
# 
# ifeq ($(OPENMP), 1)
# CFLAGS+= -fopenmp
# LDFLAGS+= -lgomp
# endif
# 
# ifeq ($(GPU), 1)
# COMMON+= -DGPU -I/usr/local/cuda/include/
# CFLAGS+= -DGPU
# ifeq ($(OS),Darwin) #MAC
# LDFLAGS+= -L/usr/local/cuda/lib -lcuda -lcudart -lcublas -lcurand
# else
# LDFLAGS+= -L/usr/local/cuda/lib64 -lcuda -lcudart -lcublas -lcurand
# endif
# endif
# 
# ifeq ($(CUDNN), 1)
# COMMON+= -DCUDNN
# ifeq ($(OS),Darwin) #MAC
# CFLAGS+= -DCUDNN -I/usr/local/cuda/include
# LDFLAGS+= -L/usr/local/cuda/lib -lcudnn
# else
# CFLAGS+= -DCUDNN -I/usr/local/cudnn/include
# LDFLAGS+= -L/usr/local/cudnn/lib64 -lcudnn
# endif
# endif
# 
# ifeq ($(CUDNN_HALF), 1)
# COMMON+= -DCUDNN_HALF
# CFLAGS+= -DCUDNN_HALF
# ARCH+= -gencode arch=compute_70,code=[sm_70,compute_70]
# endif
# 
# ifeq ($(ZED_CAMERA), 1)
# CFLAGS+= -DZED_STEREO -I/usr/local/zed/include
# ifeq ($(ZED_CAMERA_v2_8), 1)
# LDFLAGS+= -L/usr/local/zed/lib -lsl_core -lsl_input -lsl_zed
# #-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0
# else
# LDFLAGS+= -L/usr/local/zed/lib -lsl_zed
# #-lstdc++ -D_GLIBCXX_USE_CXX11_ABI=0
# endif
# endif
# 
# OBJ=image_opencv.o http_stream.o gemm.o utils.o dark_cuda.o convolutional_layer.o list.o image.o activations.o im2col.o col2im.o blas.o crop_layer.o dropout_layer.o maxpool_layer.o softmax_layer.o data.o matrix.o network.o connected_layer.o cost_layer.o parser.o option_list.o darknet.o detection_layer.o captcha.o route_layer.o writing.o box.o nightmare.o normalization_layer.o avgpool_layer.o coco.o dice.o yolo.o detector.o layer.o compare.o classifier.o local_layer.o swag.o shortcut_layer.o activation_layer.o rnn_layer.o gru_layer.o rnn.o rnn_vid.o crnn_layer.o demo.o tag.o cifar.o go.o batchnorm_layer.o art.o region_layer.o reorg_layer.o reorg_old_layer.o super.o voxel.o tree.o yolo_layer.o gaussian_yolo_layer.o upsample_layer.o lstm_layer.o conv_lstm_layer.o scale_channels_layer.o sam_layer.o
# ifeq ($(GPU), 1)
# LDFLAGS+= -lstdc++
# OBJ+=convolutional_kernels.o activation_kernels.o im2col_kernels.o col2im_kernels.o blas_kernels.o crop_layer_kernels.o dropout_layer_kernels.o maxpool_layer_kernels.o network_kernels.o avgpool_layer_kernels.o
# endif
# 
# OBJS = $(addprefix $(OBJDIR), $(OBJ))
# DEPS = $(wildcard src/*.h) Makefile include/darknet.h
# 
# all: $(OBJDIR) backup results setchmod $(EXEC) $(LIBNAMESO) $(APPNAMESO)
# 
# ifeq ($(LIBSO), 1)
# CFLAGS+= -fPIC
# 
# $(LIBNAMESO): $(OBJDIR) $(OBJS) include/yolo_v2_class.hpp src/yolo_v2_class.cpp
# 	$(CPP) -shared -std=c++11 -fvisibility=hidden -DLIB_EXPORTS $(COMMON) $(CFLAGS) $(OBJS) src/yolo_v2_class.cpp -o $@ $(LDFLAGS)
# 
# $(APPNAMESO): $(LIBNAMESO) include/yolo_v2_class.hpp src/yolo_console_dll.cpp
# 	$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -o $@ src/yolo_console_dll.cpp $(LDFLAGS) -L ./ -l:$(LIBNAMESO)
# endif
# 
# $(EXEC): $(OBJS)
# 	$(CPP) -std=c++11 $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS)
# 
# $(OBJDIR)%.o: %.c $(DEPS)
# 	$(CC) $(COMMON) $(CFLAGS) -c $< -o $@
# 
# $(OBJDIR)%.o: %.cpp $(DEPS)
# 	$(CPP) -std=c++11 $(COMMON) $(CFLAGS) -c $< -o $@
# 
# $(OBJDIR)%.o: %.cu $(DEPS)
# 	$(NVCC) $(ARCH) $(COMMON) --compiler-options "$(CFLAGS)" -c $< -o $@
# 
# $(OBJDIR):
# 	mkdir -p $(OBJDIR)
# backup:
# 	mkdir -p backup
# results:
# 	mkdir -p results
# setchmod:
# 	chmod +x *.sh
# 
# .PHONY: clean
# 
# clean:
# 	rm -rf $(OBJS) $(EXEC) $(LIBNAMESO) $(APPNAMESO)

# Commented out IPython magic to ensure Python compatibility.
# darnket사용을 위한 GPU 설정 makefile 반영

# %cd /content/darknet/
!sed -i 's/OPENCV=0/OPENCV=1/g' Makefile
!sed -i 's/GPU=0/GPU=1/g' Makefile
!sed -i 's/CUDNN=0/CUDNN=1/g' Makefile
!sed -i "s/ARCH= -gencode arch=compute_60,code=sm_60/ARCH= ${ARCH_VALUE}/g" Makefile
!make

# Commented out IPython magic to ensure Python compatibility.
# 훈련된 yolov4의 가중치를 다운 받음 
# %cd /content/darknet
!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137

#customer dataset  가져오기
!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="GCrXpvz5fbZWJP0V1hxN")
project = rf.workspace("aiteamproject01-jl11a").project("pets-ajnuw")
dataset = project.version(5).download("darknet")

#customer datset 이 저자오딘 부분 확인
dataset.location

# Commented out IPython magic to ensure Python compatibility.
#Darknknet 파일에서 customer datset이 위차한 영역에 대한 설정 
# %cd /content/darknet/
# %cp {dataset.location}/train/_darknet.labels data/obj.names
# %mkdir data/obj
#copy image and labels
# %cp {dataset.location}/train/*.jpg data/obj/
# %cp {dataset.location}/valid/*.jpg data/obj/

# %cp {dataset.location}/train/*.txt data/obj/
# %cp {dataset.location}/valid/*.txt data/obj/

with open('data/obj.data', 'w') as out:
  out.write('classes = 3\n')
  out.write('train = data/train.txt\n')
  out.write('valid = data/valid.txt\n')
  out.write('names = data/obj.names\n')
  out.write('backup = backup/')

#write train file (just the image list)
import os

with open('data/train.txt', 'w') as out:
  for img in [f for f in os.listdir(dataset.location + '/train') if f.endswith('jpg')]:
    out.write('data/obj/' + img + '\n')

#write the valid file (just the image list)
import os

with open('data/valid.txt', 'w') as out:
  for img in [f for f in os.listdir(dataset.location + '/valid') if f.endswith('jpg')]:
    out.write('data/obj/' + img + '\n')

#Custom Training Config for YOLOv4

#YOLOv4을 사용자 정의 데이터 기반으로 재구성 
#we build iteratively from base config files. This is the same file shape as cfg/yolo-obj.cfg
#dranket은 에포크관점이 아닌 얼마나 실험을 오래할것인가에 대한 반복관점이 존재함
def file_len(fname):
  with open(fname) as f:
    for i, l in enumerate(f):
      pass
  return i + 1

num_classes = file_len(dataset.location + '/train/_darknet.labels')
print("writing config for a custom YOLOv4 detector detecting number of classes: " + str(num_classes))

#Instructions from the darknet repo
#change line max_batches to (classes*2000 but not less than number of training images, and not less than 6000), f.e. max_batches=6000 if you train for 3 classes
#change line steps to 80% and 90% of max_batches, f.e. steps=4800,5400
if os.path.exists('./cfg/custom-yolov4-detector.cfg'): os.remove('./cfg/custom-yolov4-detector.cfg')


with open('./cfg/custom-yolov4-detector.cfg', 'a') as f:
  f.write('[net]' + '\n')
  f.write('batch=64' + '\n')
  #####smaller subdivisions help the GPU run faster. 12 is optimal, but you might need to change to 24,36,64####
  f.write('subdivisions=24' + '\n')
  f.write('width=416' + '\n')
  f.write('height=416' + '\n')
  f.write('channels=3' + '\n')
  f.write('momentum=0.949' + '\n')
  f.write('decay=0.0005' + '\n')
  f.write('angle=0' + '\n')
  f.write('saturation = 1.5' + '\n')
  f.write('exposure = 1.5' + '\n')
  f.write('hue = .1' + '\n')
  f.write('\n')
  f.write('learning_rate=0.001' + '\n')
  f.write('burn_in=1000' + '\n')
  ######you can adjust up and down to change training time#####
  ##Darknet does iterations with batches, not epochs####
  # max_batches = num_classes*2000
  max_batches = 2000
  f.write('max_batches=' + str(max_batches) + '\n')
  f.write('policy=steps' + '\n')
  steps1 = .8 * max_batches
  steps2 = .9 * max_batches
  f.write('steps='+str(steps1)+','+str(steps2) + '\n')

#Instructions from the darknet repo
#change line classes=80 to your number of objects in each of 3 [yolo]-layers:
#change [filters=255] to filters=(classes + 5)x3 in the 3 [convolutional] before each [yolo] layer, keep in mind that it only has to be the last [convolutional] before each of the [yolo] layers.

  with open('cfg/yolov4-custom2.cfg', 'r') as f2:
    content = f2.readlines()
    for line in content:
      f.write(line)    
    num_filters = (num_classes + 5) * 3
    f.write('filters='+str(num_filters) + '\n')
    f.write('activation=linear')
    f.write('\n')
    f.write('\n')
    f.write('[yolo]' + '\n')
    f.write('mask = 0,1,2' + '\n')
    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\n')
    f.write('classes=' + str(num_classes) + '\n')

  with open('cfg/yolov4-custom3.cfg', 'r') as f3:
    content = f3.readlines()
    for line in content:
      f.write(line)    
    num_filters = (num_classes + 5) * 3
    f.write('filters='+str(num_filters) + '\n')
    f.write('activation=linear')
    f.write('\n')
    f.write('\n')
    f.write('[yolo]' + '\n')
    f.write('mask = 3,4,5' + '\n')
    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\n')
    f.write('classes=' + str(num_classes) + '\n')

  with open('cfg/yolov4-custom4.cfg', 'r') as f4:
    content = f4.readlines()
    for line in content:
      f.write(line)    
    num_filters = (num_classes + 5) * 3
    f.write('filters='+str(num_filters) + '\n')
    f.write('activation=linear')
    f.write('\n')
    f.write('\n')
    f.write('[yolo]' + '\n')
    f.write('mask = 6,7,8' + '\n')
    f.write('anchors = 12, 16, 19, 36, 40, 28, 36, 75, 76, 55, 72, 146, 142, 110, 192, 243, 459, 401' + '\n')
    f.write('classes=' + str(num_classes) + '\n')
    
  with open('cfg/yolov4-custom5.cfg', 'r') as f5:
    content = f5.readlines()
    for line in content:
      f.write(line)

print("file is written!")

# Commented out IPython magic to ensure Python compatibility.
#here is the file that was just written. 
#you may consider adjusting certain things

#like the number of subdivisions 64 runs faster but Colab GPU may not be big enough
#if Colab GPU memory is too small, you will need to adjust subdivisions to 16
# %cat cfg/custom-yolov4-detector.cfg

#Train Custom YOLOv4 Detector

!./darknet detector train data/obj.data cfg/custom-yolov4-detector.cfg yolov4.conv.137 -dont_show -map

# Infer Custom Objects with Saved YOLOv4 Weights

# Commented out IPython magic to ensure Python compatibility.

#define utility function
def imShow(path):
  import cv2
  import matplotlib.pyplot as plt
#   %matplotlib inline
#이미지 전처리
  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)

  fig = plt.gcf()
  fig.set_size_inches(18, 10)
  plt.axis("off")
  #plt.rcParams['figure.figsize'] = [10, 5]
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()

!ls backup
#if it is empty you haven't trained for long enough yet, you need to train for at least 100 iterations
#가중치가 훈련된 백업 데이터 셋을 확인



#가중치를 불러서 test해보기 
test_images = [f for f in os.listdir('test') if f.endswith('.jpg')]
import random
img_path = "test/" + random.choice(test_images);

#test out our detector!
!./darknet detect cfg/custom-yolov4-detector.cfg backup/custom-yolov4-detector_final.weights {img_path} -dont-show
imShow('/content/darknet/predictions.jpg')

import cv2
import numpy as np
import time # -- 프레임 계산을 위해 사용


vedio_path = './video.mp4' #-- 사용할 영상 경로
min_confidence = 0.5

def detectAndDisplay(frame):
    start_time = time.time()
    img = cv2.resize(frame, None, fx=0.8, fy=0.8)
    height, width, channels = img.shape
    #cv2.imshow("Original Image", img)

    #-- 창 크기 설정
    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)

    net.setInput(blob)
    outs = net.forward(output_layers)

    #-- 탐지한 객체의 클래스 예측 
    class_ids = []
    confidences = []
    boxes = []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            #-- 원하는 class id 입력 / coco.names의 id에서 -1 할 것 
            if class_id == 0 and confidence > min_confidence:
                #-- 탐지한 객체 박싱
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
               
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, min_confidence, 0.4)
    font = cv2.FONT_HERSHEY_DUPLEX
    for i in range(len(boxes)):
        if i in indexes:
            x, y, w, h = boxes[i]
            label = "{}: {:.2f}".format(classes[class_ids[i]], confidences[i]*100)
            print(i, label)
            color = colors[i] #-- 경계 상자 컬러 설정 / 단일 생상 사용시 (255,255,255)사용(B,G,R)
            cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
            cv2.putText(img, label, (x, y - 5), font, 1, color, 1)
    end_time = time.time()
    process_time = end_time - start_time
    print("=== A frame took {:.3f} seconds".format(process_time))
    cv2.imshow("YOLO4 test", img)
    
#-- yolo 포맷 및 클래스명 불러오기
model_file = '/content/darknet/backup/custom-yolov4-detector_final.weights' #-- 본인 개발 환경에 맞게 변경할 것
config_file = '/content/darknet/cfg/custom-yolov4-detector.cfg' #-- 본인 개발 환경에 맞게 변경할 것

net = cv2.dnn.readNet(model_file, config_file)

#-- 클래스(names파일) 오픈 / 본인 개발 환경에 맞게 변경할 것
classes = []
with open("/content/darknet/data/obj.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]
layer_names = net.getLayerNames()
output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]
colors = np.random.uniform(0, 255, size=(len(classes), 3))

#-- 비디오 활성화
cap = cv2.VideoCapture(vedio_path) #-- 웹캠 사용시 vedio_path를 0 으로 변경
if not cap.isOpened:
    print('--(!)Error opening video capture')
    exit(0)
while True:
    ret, frame = cap.read()
    if frame is None:
        print('--(!) No captured frame -- Break!')
        break
    detectAndDisplay(frame)
    #-- q 입력시 종료
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cv2.destroyAllWindows()